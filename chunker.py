import time
from typing import Optional, List
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.node_parser import SemanticSplitterNodeParser, SentenceSplitter
from llama_index.core import Document

# ì „ì—­ ìºì‹œ - ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œ
_cached_embed_model = None
_cached_model_name = None

def get_embedding_model(model_name: str = "sentence-transformers/all-MiniLM-L6-v2", use_cache: bool = True):
    """
    ì„ë² ë”© ëª¨ë¸ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (ìºì‹± ì§€ì›)
    
    ë¹ ë¥¸ ëª¨ë¸ ì˜µì…˜ë“¤:
    - "sentence-transformers/all-MiniLM-L6-v2" (ê¸°ë³¸) - ë§¤ìš° ë¹ ë¦„, ì ë‹¹í•œ í’ˆì§ˆ
    - "sentence-transformers/paraphrase-MiniLM-L6-v2" - ë¹ ë¦„, ì¢‹ì€ í’ˆì§ˆ
    - "BAAI/bge-small-en-v1.5" - ë¹ ë¦„, ì¢‹ì€ í’ˆì§ˆ (ì˜ì–´)
    - "BAAI/bge-m3" (ì›ë³¸) - ëŠë¦¼, ìµœê³  í’ˆì§ˆ
    """
    global _cached_embed_model, _cached_model_name
    
    if use_cache and _cached_embed_model is not None and _cached_model_name == model_name:
        return _cached_embed_model
    
    print(f"ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
    start_time = time.time()
    
    embed_model = HuggingFaceEmbedding(
        model_name=model_name,
        trust_remote_code=True,
        device="cpu"  # GPU ì‚¬ìš© ì‹œ "cuda"ë¡œ ë³€ê²½
    )
    
    load_time = time.time() - start_time
    print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({load_time:.2f}ì´ˆ)")
    
    if use_cache:
        _cached_embed_model = embed_model
        _cached_model_name = model_name
    
    return embed_model

def split_text_into_chunks_fast(
    text: str, 
    buffer_size: int = 2, 
    breakpoint_percentile_threshold: int = 95,
    model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
    max_chunk_size: int = 1000,
    overlab_size: int = 200,
)-> list[dict]:
    """
    ë¹ ë¥¸ ì˜ë¯¸ê¸°ë°˜ ì²­í‚¹ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ (ìµœì í™”ëœ ë²„ì „)
    
    Args:
        text: ì²­í‚¹í•  í…ìŠ¤íŠ¸
        buffer_size: ì˜ë¯¸ ìœ ì‚¬ì„±ì„ í‰ê°€í•  ë•Œ í•¨ê»˜ ê·¸ë£¹í™”í•  ë¬¸ì¥ì˜ ìˆ˜ (ê¸°ë³¸ê°’: 2)
        breakpoint_percentile_threshold: ì½”ì‚¬ì¸ ë¹„ìœ ì‚¬ì„±ì˜ ë°±ë¶„ìœ„ìˆ˜ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 95)
        model_name: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ (ë¹ ë¥¸ ëª¨ë¸ ê¸°ë³¸ê°’)
        max_chunk_size: í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ì—ì„œ 1ì°¨ ë¶„í•  í¬ê¸°
        use_hybrid: í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ ì‚¬ìš© ì—¬ë¶€ (ê¸¸ì´ ê¸°ë°˜ + ì˜ë¯¸ ê¸°ë°˜)
    """
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    text = "\n".join(line.strip() for line in text.split("\n") if line.strip())
    
    print("ğŸ”€ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ ì‚¬ìš©: ê¸¸ì´ ê¸°ë°˜ 1ì°¨ ë¶„í•  + ì˜ë¯¸ ê¸°ë°˜ 2ì°¨ ë¶„í• ")
    return _hybrid_chunking(text, buffer_size, breakpoint_percentile_threshold, model_name, max_chunk_size, overlab_size)

def _hybrid_chunking(
    text: str, 
    buffer_size: int, 
    breakpoint_percentile_threshold: int, 
    model_name: str,
    max_chunk_size: int,
    overlab_size:int
) -> List[dict]:
    """í•˜ì´ë¸Œë¦¬ë“œ ì²­í‚¹: ê¸¸ì´ ê¸°ë°˜ 1ì°¨ ë¶„í•  í›„ ì˜ë¯¸ ê¸°ë°˜ 2ì°¨ ë¶„í• """
    
    # 1ì°¨: ê¸¸ì´ ê¸°ë°˜ ë¶„í• 
    sentence_splitter = SentenceSplitter(chunk_size=max_chunk_size, chunk_overlap=overlab_size)
    initial_chunks = sentence_splitter.split_text(text)
    
    print(f"ğŸ“ 1ì°¨ ê¸¸ì´ ê¸°ë°˜ ë¶„í• : {len(initial_chunks)}ê°œ ì²­í¬")
    
    if len(initial_chunks) <= 1:
        return initial_chunks
    
    # 2ì°¨: ê° ì²­í¬ì— ëŒ€í•´ ì˜ë¯¸ ê¸°ë°˜ ë¶„í• 
    embed_model = get_embedding_model(model_name)
    parser = SemanticSplitterNodeParser(
        embed_model=embed_model,
        buffer_size=buffer_size,
        breakpoint_percentile_threshold=breakpoint_percentile_threshold
    )
    
    final_chunks = []
    for i, chunk in enumerate(initial_chunks):
        print(f"ğŸ” 2ì°¨ ì˜ë¯¸ ê¸°ë°˜ ë¶„í•  ({i+1}/{len(initial_chunks)})")
        
        if len(chunk) < 200:  # ë„ˆë¬´ ì‘ì€ ì²­í¬ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            final_chunks.append(chunk)
            continue
            
        document = Document(text=chunk)
        nodes = parser.get_nodes_from_documents([document])
        final_chunks.extend([node.text for node in nodes])
    
    return final_chunks


def benchmark_models(sample_text: str):
    """ë‹¤ì–‘í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë²¤ì¹˜ë§ˆí¬í•˜ëŠ” í•¨ìˆ˜"""
    models = [
        "sentence-transformers/all-MiniLM-L6-v2",  # ê°€ì¥ ë¹ ë¦„
        "sentence-transformers/paraphrase-MiniLM-L6-v2",  # ë¹ ë¦„
        "BAAI/bge-small-en-v1.5",  # ì¤‘ê°„
        "BAAI/bge-m3"  # ëŠë¦¼í•˜ì§€ë§Œ ê³ í’ˆì§ˆ
    ]
    
    results = {}
    for model in models:
        print(f"\nğŸ§ª ë²¤ì¹˜ë§ˆí¬: {model}")
        start_time = time.time()
        try:
            chunks = split_text_into_chunks_fast(
                sample_text, 
                model_name=model,
                use_hybrid=True
            )
            elapsed = time.time() - start_time
            results[model] = {
                "time": elapsed,
                "chunks": len(chunks),
                "success": True
            }
            print(f"âœ… ì™„ë£Œ: {elapsed:.2f}ì´ˆ, {len(chunks)}ê°œ ì²­í¬")
        except Exception as e:
            results[model] = {
                "time": None,
                "chunks": 0,
                "success": False,
                "error": str(e)
            }
            print(f"âŒ ì‹¤íŒ¨: {e}")
    
    return results